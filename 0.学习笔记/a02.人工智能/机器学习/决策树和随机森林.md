## 决策树

决策树是一种有监督的机器学习算法，该方法可以用于解决分类和回归问题。决策树可以简单地理解为达到某一特定结果的一系列决策。

### 数据集

数据集，包含150个鸢尾花样本数据，数据特征包含花瓣的长度和宽度和萼片的长度和宽度，包含三个属种的鸢尾花，分别是山鸢尾(setosa)、变色鸢尾(versicolor)和维吉尼亚鸢尾(virginica)。

- **特征**：1. 花萼长度 2. 花萼宽度 3. 花瓣长度 4 花萼宽度
- **标签**：种类：山鸢尾(setosa)、变色鸢尾(versicolor)和维吉尼亚鸢尾(virginica)

比如想对这个数据集按照以上的给定特征进行分类

用seaborn绘制花瓣长度和宽度特征对应鸢尾花种类的散点图

![image](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/bVcOpl7)

当花瓣长度小于2.45则为山鸢尾(setosa)，剩下的我们判断花瓣宽度小于1.75则为变色鸢尾(versicolor)剩下的为维吉尼亚鸢尾(virginica)。那么我用导图画一下这种判别式的树形结构如下：

![image](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/bVcO0jh)

这种决策是由人工判断的，而决策树要做的就是代替人工来生成判断树

### 决策树如何确定特征的判断顺序

特征重要性和特质的检测顺序是基于如信息增益或基尼不纯度指数等标准来决定的。

#### 信息熵

在信息论中，熵表示随机变量不确定性的度量。

假设离散随机变量X的概率分布是P(X),则其熵是：
![image-20220615142443137](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220615142443137.png)

假设当前样本集合D中的k类样本所占的比例为$p_k$，则其D的信息熵为：

![image-20220615223218995](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220615223218995.png)

#### 条件熵

**条件熵**  **，表示引入随机变量Y对于消除X不确定性的程度**。假如X、Y相互独立，则X的条件熵和熵有相同的值；否则条件熵一定小于熵。

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/v2-92cf8f4aa7e317cb825414cf703fd582_720w.jpg)

#### 信息增益

熵 - 条件熵。表示在一个条件下，信息不确定性减少的程度。

![image-20220615143805506](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220615143805506.png)

**在决策树算法中，我们的关键就是每次选择一个特征，特征有多个，那么到底按照什么标准来选择哪一个特征。**

这个问题就可以用信息增益来度量。**如果选择一个特征后，信息增益最大（信息不确定性减少的程度最大）**，那么我们就**选取这个特征。**

这个就是**ID3算法**使用的策略

#### 信息增益率（gain ratio）

信息增益有明显的弱点，会对可取值数目较多的属性有所偏好，比如将编号作为一个属性，所有样本的编号均不相同，每个样本分成一类的熵最小，纯度最大，但是这个是不合理的，于是引入增益率：

![image-20220615151736235](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220615151736235.png)

启发式：先从候选划分属性中找到信息增益高于平均水平的，再从中选取增益率最高的

这个就是C4.5算法中使用的策略

#### 基尼系数

![image-20220615151859441](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220615151859441.png)

![image-20220615155013814](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220615155013814.png)

注：属性a的基尼系数就是数据集D按照属性a分出的多个数据集的基尼系数之和

这个就是CART算法中的策略

### 防止过拟合

#### 产生过拟合数据问题的原因

##### 样本问题

（1）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；

（2）样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景；

（3）建模时使用了样本中太多无关的输入变量。

##### 构建决策树的方法问题

在决策树模型搭建中，我们使用的算法对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据或非事件数据，可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。

#### 解决过拟合问题

剪枝：提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝

##### 先剪枝

通过提前停止树的构建而对树“剪枝”，一旦停止，节点就成为树叶。该树叶可以持有子集元组中最频繁的类；

1. 定义一个高度，当决策树达到该高度时就可以停止决策树的生长，这是一种最为简单的方法；
2. 达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这种方法对于处理数据中的数据冲突问题非常有效；
3. 定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长；
4. 定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。

##### 后剪枝

它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。后剪枝的剪枝过程是删除一些子树，然后用其叶子节点代替，这个叶子节点所标识的类别通过大多数原则(majority class criterion)确定。所谓大多数原则，是指剪枝过程中, 将一些子树删除而用叶节点代替,这个叶节点所标识的类别用这棵子树中大多数训练样本所属的类别来标识,所标识的类称为majority class .相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。

1)**REP方法**是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个**训练集用来形成学习到的决策树**，一个**分离的验证集用来评估这个决策树在后续数据上的精度**，确切地说是用来评估修剪这个决策树的影响。这个方法的动机是：即使学习器可能会被训练集中的随机错误和巧合规律所误导，但验证集合不大可能表现出同样的随机波动。所以验证集可以用来对过度拟合训练集中的虚假特征提供防护检验。

该剪枝方法考虑将书上的每个节点作为修剪的候选对象，决定是否修剪这个结点有如下步骤组成：

1. 删除以此结点为根的子树
2. 使其成为叶子结点
3. 赋予该结点关联的训练数据的最常见分类
4. 当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点
   因为训练集合的过拟合，使得验证集合数据能够对其进行修正，反复进行上面的操作，从底向上的处理结点，删除那些能够最大限度的提高验证集合的精度的结点，直到进一步修剪有害为止(有害是指修剪会减低验证集合的精度)。

REP是最简单的后剪枝方法之一，不过由于使用独立的测试集，原始决策树相比，修改后的决策树可能偏向于过度修剪。这是因为一些不会再测试集中出现的很稀少的训练集实例所对应的分枝在剪枝过如果训练集较小，通常不考虑采用REP算法。
尽管REP有这个缺点，不过REP仍然作为一种基准来评价其它剪枝算法的性能。它对于两阶段决策树学习方法的优点和缺点提供了了一个很好的学习思路。由于验证集合没有参与决策树的创建，所以用REP剪枝后的决策树对于测试样例的偏差要好很多，能够解决一定程度的过拟合问题。

2)**PEP,悲观错误剪枝,**悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪。该方法引入了统计学上连续修正的概念弥补REP中的缺陷，在评价子树的训练错误公式中添加了一个常数，假定每个叶子结点都自动对实例的某个部分进行错误的分类。它不需要像REP(错误率降低修剪)样，需要用部分样本作为测试数据，而是完全使用训练数据来生成决策树，又用这些训练数据来完成剪枝。决策树生成和剪枝都使用训练集, 所以会产生错分。

   ***把一棵子树（具有多个叶子节点）的分类用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在测试数据上不一定，我们需要把子树的误判计算加上一个经验性的惩罚因子，用于估计它在测试数据上的误判率。***对于一棵叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为（E+0.5）/N。这个0.5就是惩罚因子，那么对于该棵子树，假设它有L个叶子节点，则该子树的误判率估计为:

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20171206194451321)

剪枝后该子树内部节点变成了叶子节点，该叶子结点的误判个数J同样也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5在

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20171206194600979)

的标准误差内。对于样本的误差率e，我们可以根据经验把它估计成伯努利分布，那么可以估计出该子树的误判次数均值和标准差

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20171206194911237)

使用训练数据，子树总是比替换为一个叶节点后产生的误差小，但是使用校正的误差计算方法却并非如此。

剪枝的条件:当子树的误判个数大过对应叶节点的误判个数一个标准差之后，就决定剪枝：

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20171206195241370)

这个条件就是剪枝的标准。当然并不一定非要大一个标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。

### 决策树算法实现

![在这里插入图片描述](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20210516111520857.png)

## 随机森林

![在这里插入图片描述](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20200713103223688.png)

随机森林是基于树的机器学习算法，该算法利用了多棵决策树的力量来进行决策。为什么要称其为“随机森林”呢？这是因为它是随机创造的决策树组成的森林。决策树中的每一个节点是特征的一个随机子集，用于计算输出。随机森林将单个决策树的输出整合起来生成最后的输出结果。简单来说：“随机森林算法用多棵（随机生成的）决策树来生成最后的输出结果。

**这种结合了多个单一模型的输出（也被称为弱学习）的过程被称为集成学习。**

随机体现在两个方面：

1. **随机选择样本**：首先，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。

   ![在这里插入图片描述](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20200713103816550.png)

2. **随机选择特征**：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。下图中，蓝色的方块代表所有可以被选择的特征，也就是待选特征。黄色的方块是分裂特征。左边是一棵决策树的特征选取过程，通过在待选特征中选取最优的分裂特征（ID3算法，C4.5算法，CART算法等等），完成分裂。右边是一个随机森林中的子树的特征选取过程。

   ![在这里插入图片描述](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20200713103851504.png)
