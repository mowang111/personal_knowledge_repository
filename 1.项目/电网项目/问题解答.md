## 可逆与不可逆

按照压缩过程对网络结构的破坏程度，将模型压缩技术分为“前端压缩”与“后端压缩”两部分，后端压缩为了追求极致的压缩比，对原有的网络结构进行改造，从结果上可能会影响原有网络结构，所以是不可逆的，而前端压缩不改变原网络结构，根据已有结果决定参数的部分修改，不改变原有结果，所以是可逆的。

## 低秩近似

### 奇异值分解

矩阵A是一个m×n的矩阵，定义矩阵A的SVD为：

$A=UΣV^T$

其中U是一个$m×m$的矩阵，Σ是一个$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个n×n的矩阵。

对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵

**矩阵的SVD分解可以看作 左右奇异向量外积所得矩阵+对应的权重（也就是对应奇异值）所得的一系列矩阵之和。**

### CP分解

一个三维张量的CP分解：

![这里写图片描述](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20180825185631219)

对于一个 ![[公式]](https://www.zhihu.com/equation?tex=\chi+\in+R^{I\times+J\times+K}) ，其CP分解公式如下：

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/v2-6d84257da1ce71731067efb16b8a358e_720w.jpg)

**CP分解可以看作奇异值分解的高阶形式，奇异值分解是对于矩阵而言，而CP分解是对于张量而言**

**张量的CP分解是将张量分解为许多因子张量之和**

### Tucker分解

**Tucker分解是主成成分分析（PCA）的高阶形式**。（PCA的目的就是**用** ![[公式]](https://www.zhihu.com/equation?tex=k) **个主分量概括表达统计相关的 ![[公式]](https://www.zhihu.com/equation?tex=n) 个特征**，因此PCA算法是一种降维方法)

**CP分解可以看作Tucker分解的特殊情况**（**如果核心张量是一个超对角张量的话**，**那么Tucker分解就变成了CP分解**）

对于张量 ![[公式]](https://www.zhihu.com/equation?tex=\chi+\in+R^{I\times+J\times+K}) ，其Tucker分解形式如下：

![img](https://pic4.zhimg.com/80/v2-722cfc4cbc1c35ec13204c6b5075b9bf_720w.png)

其中， ![[公式]](https://www.zhihu.com/equation?tex=A+\in+R^{I+\times+P}%2C+B+\in+R^{J+\times+Q}%2C+C+\in+R^{K+\times+R}) 是Tucker分解得到的**因子矩阵**，

![img](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/v2-3ca6cf050641d99e0741254268d13711_720w.png)

代表**核心张量，它的元素代表不同因子矩阵之间相互作用的水平**。Tucker分解示意图如下：

![img](https://pic4.zhimg.com/80/v2-6b29ac8c75c692535a0baf19b231466f_720w.jpg)

### Tensor Train分解

当我们用**严格对角化算法**的时候，哈密顿量具有特殊的求和结构，但基态**指数的复杂度**是不能避免的。
当系统包含的自旋个数N N*N*增加时，量子态系数的个数随N N*N*指数增加（参数复杂度随着N N指数增加），将无法使用经典计算机实现之前介绍的严格算法进行计算（**“指数墙”**）

**Tensor-train Decomposition将原来的高维张量分解为多个三维张量的乘积（首尾张量为二维）**

给定一个N阶张量，将其分解成N个二阶或三阶张量的缩并形式：

![image-20220128024325131](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220128024325131.png)

![在这里插入图片描述](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/20201209181128602.png)

可通过**N-1次**的**奇异值分解**或**QR分解**实现
但是，如果不作任何**近似**的TT分解（即在**不亏秩**的情况下)不能解决“指数墙”问题。

我们发现:中间辅助指标的维数会随着分解的次数而指数上升

**如何解决？——于是引出了我们的TT分解的低秩近似。**

TT秩：我们进行n-1次奇异值分解时，每次SVD都会产生一个奇异谱，奇异谱中非零奇异值的个数就是本次矩阵的秩。n-1次奇异值分解后得到n-1个数，即为TT秩。
注：在不亏秩的情况下，TT秩就相当于几何指标的维数
![image-20220128023745954](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220128023745954.png)

### Block Term分解

每一个成员张量是一个$Rank−(Lr,Mr,Nr)$的Tucker分解。

![Block term decompositon(BTD)](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/BTD.png)

采用数学表达式可以写为：

![image-20220126215722994](https://raw.githubusercontent.com/mowang111/image-hosting/master/typora_images/image-20220126215722994.png)



其中 $Dr$是一个$Rank−(Lr,Mr,Nr)$的张量，$Ar∈RI×L$是一个秩为I的矩阵。**BTD分解可以看作是我们之前介绍的Tucker分解和CP分解的结合形式。**

**当R=1的时候，显然成员张量只有一个，此时这个分解就是个Rank−(L,M,N)的Tucker分解。**

**而当每一个成员张量是一个Rank−1分解的时候，该分解退化为一个CP分解 (CP分解是将一个张量分解为R个 秩为1的张量分解形式)。**

